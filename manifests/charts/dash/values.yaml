image:
  registry: ghcr.io
  repository: m9sweeper/dash
  tag: latest
  pullPolicy: Always
  credentials:
    create: false
    username: ""
    password: ""
    secretRef: ""

kubesec:
  registry: docker.io
  repository: kubesec/kubesec
  tag: v2.13.0
  pullPolicy: Always
  credentials:
    create: false
    username: ""
    password: ""
    secretRef: ""

# Creates another deployment of M9sweeper, a service, and alters the ingress.
# All requests sent to /api/falco will route to this instance. Useful for environments with many Falco alerts.
falco:
  bulkhead: false
  replicas: 1

nameOverride: ""
fullnameOverride: ""

resources:
  limits:
    cpu: 300m
    memory: 512Mi
  requests:
    cpu: 50m
    memory: 128Mi

service:
  type: ClusterIP
  port: 3000

# Creates kube-prometheus-stack CRDs for dashboards, scrapers, and alerts
prometheusOperator:
  enabled: true
  scrapeInterval: "60s"
  defaultRules:
    enabled: true
    # Alert if more than X new unique CVEs discovered
    numNewCVEs:
      enabled: false
      threshold: 20
      duration: 10m  # defaults to 10m
    # Alerts if over X% of pods in the cluster are non-compliant with CVE standards
    percentNoncompliance:
      enabled: true
      threshold: 20
    # Alerts if there's a significant increase in number of failed tests (as run by kube-bench)
    numNewTestFailures:
      enabled: true
      threshold: 5
      duration: 6h  # defaults to 6h
    # Alerts if there's a significant increase in high-severity vulnerabilities (as detected by kube-hunter)
    numNewVulnerabilitiesDetected:
      enabled: true
      threshold: 20
      duration: 6h  # defaults to 6h
  # Convenient way of specifying additional rules
  additionalRules: {}
#    ruleName:
#      additionalLabels:
#        extraLabel: helloWorld
#      groups:
#        - name: example
#          rules:
#            - alert: test
#              annotations:
#                description: Describe here
#                runbook_url: url_here
#                summary: Summarize here
#              expr: (delta(num_critical_cves[10m])+delta(num_major_cves[10m])) > 20
#              for: 10m
#              labels:
#                severity: critical

grafana:
  enabled: true
  labels:
    grafana_dashboard: 1
  defaultDashboards:
    enabled: true

replicaCount: 1

init:
  clusterGroupName: ""
  clusterName: ""
  clusterApiKey: ""
  superAdminEmail: ""
  superAdminPassword: ""
  licenseKey: ""
  instanceKey: ""
  namespaceExceptions:
    - kube-system
  docker:
    registries:
      - name: Dockerhub
        hostname: docker.io
        aliases:
          - index.docker.io
        login_required: false
      - name: "GitHub Container Registry"
        hostname: ghcr.io
        login_required: false
      - name: "Kubernetes Container Registry"
        hostname: registry.k8s.io
        login_required: false

# validating webhook not installed by default
validatingWebhook:
  enabled: false

###########
# Ingress #
###########
# Kubernetes native ingress and Istio are supported, only enable one or neither of them.
# If you enable neither you will need to port-forward to dash
ingress:
  hosts: []
    # Add lists of hosts
    # - example.local
  path: /

  # Deploys a Kubernetes Ingress resource, defaults to nginx ingress controller.
  # If networking.k8s.io/v1 is available in your cluster m9sweeper will use that,
  # otherwise fall back to networking.k8s.io/v1
  k8sIngress:
    enabled: false
    annotations:
      # kubernetes.io/ingress.class: gce
      kubernetes.io/ingress.class: nginx
      # nginx.ingress.kubernetes.io/ssl-redirect: "false"
      # kubernetes.io/ingress.allow-http: false
      # kubernetes.io/tls-acme: true
    tls: []
      # Secrets must be manually created in the namespace.
      # - secretName: dash-certificate
      #   hosts:
      #     - example.local

  # Deploys an Istio VirtualService, DestinationRule, Gateway (optional), PeerAuthentication (optional)
  istio:
    enabled: false
    loadBalancerType: ROUND_ROBIN
    mtlsMode: "PERMISSIVE"

############
# CronJobs #
############
# Cluster scraping is done with CronJobs, enable/disable here
clusterScraping:
  # Scrapes current running images in all clusters, and queues any new images to be scanned by Trawler
  # Set schedule to scrape more often for more precise metrics
  scheduled:
    enabled: true
    schedule: "30 * * * *" # every 30 min
  # Scrapes history of images deployed for dashboards, set schedule to any time, but only once per day
  history:
    enabled: true
    schedule: "1 0 * * *" # every day 12.01 am
  # Calculating gatekeeper exception block
  gatekeeperExceptionBlock:
    enabled: true
    schedule: "30 * * * *" # every 30 min
  # Updating exception status
  updateExceptionStatus:
    enabled: true
    schedule: "1 0 * * *" # every day 12.01 am

################
# Email Config #
################
email:
  # Email method options are SMTP or SENDGRID
  method: SMTP
  # Required for SMTP method
  smtp:
    host: ""
    port: "465"
    tlsRequired: true
    user: ""
    password: ""
  # Required for SENDGRID method
  sendgridApiKey: ""
  senderEmail: ""
  # Enable/disable system error email notifications
  enableSystemErrorEmail: false
  # The email address to send system error emails to
  systemErrorMailTo: ""

################
# Extra Config #
################
nodeSelector: {}

tolerations: []

affinity: {}

# Addtional key/value pairs to be included in Dash's environment,
# which gets loaded into env vars
extraConfigVars: {}

extraSecretVars: {}
